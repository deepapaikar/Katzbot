{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7085165,"sourceType":"datasetVersion","datasetId":4082167},{"sourceId":7141125,"sourceType":"datasetVersion","datasetId":4121656},{"sourceId":7161428,"sourceType":"datasetVersion","datasetId":4132440}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n# %pip install transformers==4.30\n%pip install -U peft\n%pip install -U accelerate einops\n%pip install -U trl \n%pip install rouge_score rouge\n%pip install datasets==2.11\n%pip install -U flash-attn --no-build-isolation\n\n\n# !pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n# !pip install -q -U trl transformers accelerate \n# # !pip install git+https://github.com/huggingface/peft.git\n# !pip install -q einops wandb\n# %pip install datasets==2.11\n\n# !pip install scipy\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch, wandb\nfrom datasets import load_dataset\nimport datasets\nfrom trl import SFTTrainer\nimport pandas as pd\nimport shutil\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","metadata":{"id":"fNZZilUV_L4r","outputId":"31a2fa06-284f-411a-f439-0e8f949bfaf8","execution":{"iopub.status.busy":"2023-12-11T14:23:58.223182Z","iopub.execute_input":"2023-12-11T14:23:58.223496Z","iopub.status.idle":"2023-12-11T14:25:43.840525Z","shell.execute_reply.started":"2023-12-11T14:23:58.223470Z","shell.execute_reply":"2023-12-11T14:25:43.839473Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# !python sparseml/src/sparseml/transformers/sparsification/obcq/export.py --task text-generation --model_path obcq_deployment --sequence_length 4096\n# !cp deployment/model.onnx deployment/model-orig.onnx","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:38:39.897156Z","iopub.execute_input":"2023-12-11T13:38:39.897757Z","iopub.status.idle":"2023-12-11T13:38:39.902112Z","shell.execute_reply.started":"2023-12-11T13:38:39.897729Z","shell.execute_reply":"2023-12-11T13:38:39.901220Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nsecret_wandb = user_secrets.get_secret(\"Wandb\")\n!huggingface-cli login --token $secret_hf\n\n# Monitering the LLM\nwandb.login(key = secret_wandb)\nrun = wandb.init(\n    project='Fine tuning MPhi SCQA', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:31:11.374251Z","iopub.execute_input":"2023-12-11T13:31:11.374555Z","iopub.status.idle":"2023-12-11T13:31:47.293945Z","shell.execute_reply.started":"2023-12-11T13:31:11.374530Z","shell.execute_reply":"2023-12-11T13:31:47.292833Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrebitib636\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231211_133116-288oh7kn</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rebitib636/Fine%20tuning%20Mphi%20SCQA/runs/288oh7kn' target=\"_blank\">polished-energy-23</a></strong> to <a href='https://wandb.ai/rebitib636/Fine%20tuning%20Mphi%20SCQA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rebitib636/Fine%20tuning%20Mphi%20SCQA' target=\"_blank\">https://wandb.ai/rebitib636/Fine%20tuning%20Mphi%20SCQA</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rebitib636/Fine%20tuning%20Mphi%20SCQA/runs/288oh7kn' target=\"_blank\">https://wandb.ai/rebitib636/Fine%20tuning%20Mphi%20SCQA/runs/288oh7kn</a>"},"metadata":{}}]},{"cell_type":"code","source":"sc_data =load_dataset(\"csv\", data_files = \"/kaggle/input/scqa-data/Final_SCPairs.csv\", split=\"train[:]\", encoding='latin')\n\ntrain_qa = load_dataset(\"csv\", data_files = \"/kaggle/input/scqa-data/FInal_QAPairs.csv\", split=\"train[:]\")\ntest_qa = load_dataset(\"csv\", data_files = \"/kaggle/input/scqa-data/QA_test.csv\", split=\"train[:]\")\n\nfrom datasets import Dataset\ndef preprocess_data(data_dict, is_qa=True):\n    final_text = []\n    if is_qa:\n        for q, a in zip(data_dict['question'],data_dict['answer']):\n#             t = f\"<s>[INST] Answer the following question truthfully: {q} [/INST] {a} </s>\"\n#             t = f\"<|prompter|>Answer the following question truthfully: {q}</s><|assistant|> {a}\"\n            t = f\"question: {q} answer: {a}\"\n            final_text.append({'text': t})\n    else:\n        for q, a in zip(data_dict['Question'],data_dict['Answer']):\n#             t = f\"<s>[INST] Complete the following text: {q} [/INST] {a} </s>\"\n#             t = f\"<|prompter|>Complete the following text: {q}</s><|assistant|> {a}\" \n            t = f\"question: {q} answer: {a}\"\n            final_text.append({'text': t})\n    return final_text\n\ntrain_qa = Dataset.from_list(preprocess_data(train_qa)).with_format(\"torch\")\ntest_qa = Dataset.from_list(preprocess_data(test_qa)).with_format(\"torch\")\n\nsc_data = Dataset.from_list(preprocess_data(sc_data, is_qa=False)).train_test_split(test_size=0.025).with_format(\"torch\")\n\nprint(\"Test Sentence completion:\\n\", sc_data['test'][0], end= \"\\n\" + 20* '--' + \"\\n\")\nprint(\"Train Sentence completion:\\n\", sc_data['train'][0], end= \"\\n\" + 20* '--' + \"\\n\")\nprint(\"Train Question answer:\\n\", train_qa[0], end= \"\\n\" + 20* '--' + \"\\n\")\nprint(\"Test Question answer:\\n\", test_qa[0], end= \"\\n\" + 20* '--' + \"\\n\")\n\nprint(\"Test SC: \", len(sc_data['test']))\nprint(\"Train SC: \", len(sc_data['train']))\nprint(\"Train QA: \", len(train_qa))\nprint(\"Test QA: \", len(test_qa))\n\nfor i in range(10):\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:25:43.842330Z","iopub.execute_input":"2023-12-11T14:25:43.842897Z","iopub.status.idle":"2023-12-11T14:25:45.202218Z","shell.execute_reply.started":"2023-12-11T14:25:43.842863Z","shell.execute_reply":"2023-12-11T14:25:45.201286Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-e6b2c5048bec06d7/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fdd442be0564ef19790738967e171f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e813db798941430dbbe209e84e52d274"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-e6b2c5048bec06d7/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-9fc4df6906c3d98e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d5745727a0c4e54b8fb6b43b0eedd80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"257d68a5401a4b45b59f08b12ee1b76c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-9fc4df6906c3d98e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-c93aa09ed6e1f934/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27ff5031c2d3434c9ea02bfb7b0dce5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7ebfca7ce6045088d0667c7ae1f1272"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-c93aa09ed6e1f934/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\nTest Sentence completion:\n {'text': 'question: Were there any scholars whose classes stood out to you answer: I really did enjoy every course I took  and learned from each of my teachers'}\n----------------------------------------\nTrain Sentence completion:\n {'text': 'question: Students may apply to the S. Daniel Abraham Honors Program at Stern College for Women, the Jay and Jeanie Schottenstein Honors Program at Yeshiva College, or the Business Honors and Entrepreneurial Leadership Program at Sy Syms School of Business either directly from high school or while engaged in post-high-school study in Israel. answer: Honors students participate in advanced-level courses, engage in unique extra-curricular activities, complete an intensive senior thesis/project, and are eligible for a generous scholarship towards tuition.'}\n----------------------------------------\nTrain Question answer:\n {'text': 'question: Who is the new full-time clinical assistant professor at the Sy Syms School of Business in 2023? answer: Steve Mayer'}\n----------------------------------------\nTest Question answer:\n {'text': 'question: Which Yeshiva University school is Dr. Assa Cohen set to join as a visiting assistant professor in 2023? answer: Dr. Assa Cohen is joining the Sy Syms School of Business.'}\n----------------------------------------\nTest SC:  141\nTrain SC:  5462\nTrain QA:  5166\nTest QA:  1000\n","output_type":"stream"}]},{"cell_type":"code","source":"combined = sc_data['train']['text'] + train_qa['text']\nlist_of_dicts = [{\"text\": item} for item in combined]\n\ndataset = datasets.Dataset.from_list(list_of_dicts).with_format(\"torch\")\ncombined_train = dataset.shuffle(seed=42).with_format(\"torch\")\nlen(combined_train['text']) == len(train_qa['text'] ) + len(sc_data['train']['text'])\n\ncombined = sc_data['test']['text'] + test_qa['text']\nlist_of_dicts = [{\"text\": item} for item in combined]\n\ndataset = datasets.Dataset.from_list(list_of_dicts).with_format(\"torch\")\ncombined_test = dataset.shuffle(seed=42).with_format(\"torch\")\nlen(combined_test['text']) == len(test_qa['text'] ) + len(sc_data['test']['text'])","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:25:45.203245Z","iopub.execute_input":"2023-12-11T14:25:45.203556Z","iopub.status.idle":"2023-12-11T14:25:45.869405Z","shell.execute_reply.started":"2023-12-11T14:25:45.203507Z","shell.execute_reply":"2023-12-11T14:25:45.868435Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(  \n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= True,\n)\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=16,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n        target_modules = [ \"fc1\", 'fc2', 'linear', \"Wqkv\", \"out_proj\"]\n\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:25:45.871729Z","iopub.execute_input":"2023-12-11T14:25:45.872567Z","iopub.status.idle":"2023-12-11T14:25:45.879453Z","shell.execute_reply.started":"2023-12-11T14:25:45.872528Z","shell.execute_reply":"2023-12-11T14:25:45.878625Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"%%time\n# model_id = \"codegood/Mistral_instruct_latest\"\nmodel_id = \"microsoft/phi-1_5\"\nmodel_id = \"codegood/MPhi_Latest\"\n# model_id = \"filipealmeida/Mistral-7B-Instruct-v0.1-sharded\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        model_id,\n    load_in_4bit = True,\n        quantization_config=bnb_config,\n        torch_dtype=torch.bfloat16,\n#         use_flash_attention_2=True,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\n\nmodel.config.use_cache = False # silence the warnings. Please re-enable for inference!\nmodel.config.pretraining_tp = 1\n# model.gradient_checkpointing_enable()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, device_map=\"auto\")\ntokenizer.padding_side = 'left'\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\n# tokenizer.add_bos_token, tokenizer.add_eos_token\nmodel = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)\nfor i in range(10):\n    torch.cuda.empty_cache()\n    \n#Adding the adapters in the layers\nmodel = get_peft_model(model, peft_config)","metadata":{"id":"StJKGiDDHzdk","outputId":"871214ba-6c30-4ecf-ac68-550f296b7ef6","execution":{"iopub.status.busy":"2023-12-11T13:33:25.351041Z","iopub.execute_input":"2023-12-11T13:33:25.351407Z","iopub.status.idle":"2023-12-11T13:33:32.105421Z","shell.execute_reply.started":"2023-12-11T13:33:25.351378Z","shell.execute_reply":"2023-12-11T13:33:32.104227Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m<timed exec>:6\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3775\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3772\u001b[0m     model \u001b[38;5;241m=\u001b[39m quantizer\u001b[38;5;241m.\u001b[39mpost_init_model(model)\n\u001b[1;32m   3774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _adapter_model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3775\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3776\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_adapter_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3777\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3778\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3779\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_loading_info:\n\u001b[1;32m   3783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loading_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/integrations/peft.py:180\u001b[0m, in \u001b[0;36mPeftAdapterMixin.load_adapter\u001b[0;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, adapter_kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adapter_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    176\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter model file not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_model_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Make sure you are passing the correct path to the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m         )\n\u001b[0;32m--> 180\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m \u001b[43mPeftConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpeft_model_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Create and add fresh new adapters into the model.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m inject_adapter_in_model(peft_config, \u001b[38;5;28mself\u001b[39m, adapter_name)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/config.py:134\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m     config_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\n\u001b[1;32m    133\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclass_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloaded_attributes}\n\u001b[0;32m--> 134\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mconfig_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config\n","\u001b[0;31mTypeError\u001b[0m: LoraConfig.__init__() got an unexpected keyword argument 'loftq_config'"],"ename":"TypeError","evalue":"LoraConfig.__init__() got an unexpected keyword argument 'loftq_config'","output_type":"error"}]},{"cell_type":"code","source":"# Fine tuned model\nfrom peft import PeftConfig\nPEFT_MODEL_ID = \"codegood/MPhi_Latest\"\n# PEFT_MODEL_ID = \"codegood/Mistral_instruct_latest\"\n\n\nconfig = PeftConfig.from_pretrained(PEFT_MODEL_ID)\nmodel = AutoModelForCausalLM.from_pretrained(\n    config.base_model_name_or_path,\n    load_in_4bit = True,\n    return_dict=True,\n    quantization_config=bnb_config,\n    device_map= \"auto\",\n    trust_remote_code=True\n)\n\nmodel = PeftModel.from_pretrained(model, PEFT_MODEL_ID)\n\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,  device_map=\"auto\")\ntokenizer.pad_token = tokenizer.eos_token\nmodel = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)\n\nfor i in range(10):\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:27:29.659278Z","iopub.execute_input":"2023-12-11T14:27:29.660123Z","iopub.status.idle":"2023-12-11T14:27:47.005653Z","shell.execute_reply.started":"2023-12-11T14:27:29.660090Z","shell.execute_reply":"2023-12-11T14:27:47.004758Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"per_device_train_batch_size = 1 # reduce batch size by 2x if out-of-memory error\nper_device_eval_batch_size = 1\ngradient_accumulation_steps = 1  # increase gradient accumulation steps by 2x if batch size is reduced\noptim = \"paged_adamw_32bit\" # activates the paging for better memory management\nsave_strategy=\"steps\" # checkpoint save strategy to adopt during training\nsave_steps = 1000 # number of updates steps before two checkpoint saves\nlogging_steps = 500 # number of update steps between two logs if logging_strategy=\"steps\"\nlearning_rate = 2e-4  # learning rate for AdamW optimizer\nmax_grad_norm = 0.3 # maximum gradient norm (for gradient clipping)\nmax_steps = -1      # training will happen for 320 steps\nwarmup_ratio = 0.05 # number of steps used for a linear warmup from 0 to learning_rate\nlr_scheduler_type = \"cosine\"  # learning rate scheduler\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=per_device_train_batch_size,\n    per_device_eval_batch_size=per_device_eval_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    learning_rate=learning_rate,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    save_total_limit = 3,\n    num_train_epochs=1,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n    evaluation_strategy='steps',\n    eval_steps=save_steps,\n    metric_for_best_model=\"eval_loss\",\n    logging_steps=logging_steps,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n#     hub_model_id=\"HF_AWS_Mistral_SC\",\n    report_to=\"tensorboard\",\n    load_best_model_at_end=True,\n)\n# Setting sft parameters\ntrainer = SFTTrainer(\n    model=model,\n    eval_dataset=sc_data['test'],\n#     eval_dataset = combined_test,\n    train_dataset=sc_data['train'],\n#     train_dataset = combined_train,\n    neftune_noise_alpha=5,\n    peft_config=peft_config,\n    max_seq_length= 256,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"id":"peOnLAAhS0y1","execution":{"iopub.status.busy":"2023-12-11T14:27:47.007300Z","iopub.execute_input":"2023-12-11T14:27:47.007607Z","iopub.status.idle":"2023-12-11T14:27:47.858924Z","shell.execute_reply.started":"2023-12-11T14:27:47.007579Z","shell.execute_reply":"2023-12-11T14:27:47.857975Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:205: UserWarning: You passed a `neftune_noise_alpha` argument to the SFTTrainer, the value you passed will override the one in the `TrainingArguments`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5462 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/141 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"markdown","source":"### Sentence completion training","metadata":{}},{"cell_type":"code","source":"for i in range(10):\n    torch.cuda.empty_cache()\nmodel.config.use_cache = False\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:27:47.860220Z","iopub.execute_input":"2023-12-11T14:27:47.860611Z","iopub.status.idle":"2023-12-11T14:27:50.713440Z","shell.execute_reply.started":"2023-12-11T14:27:47.860574Z","shell.execute_reply":"2023-12-11T14:27:50.712052Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 280\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2732\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2730\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2731\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2732\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2734\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:1905\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1903\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1905\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"],"ename":"RuntimeError","evalue":"element 0 of tensors does not require grad and does not have a grad_fn","output_type":"error"}]},{"cell_type":"code","source":"model.push_to_hub(\"codegood/results\")","metadata":{"execution":{"iopub.status.busy":"2023-12-10T06:06:36.086648Z","iopub.execute_input":"2023-12-10T06:06:36.087542Z","iopub.status.idle":"2023-12-10T06:06:39.743841Z","shell.execute_reply.started":"2023-12-10T06:06:36.087500Z","shell.execute_reply":"2023-12-10T06:06:39.742632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_dir = \"/kaggle/working/results\"\nfor i in sorted(os.listdir(check_dir))[:-2]:\n    shutil.rmtree(os.path.join(check_dir, i))","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:53:20.603056Z","iopub.execute_input":"2023-12-06T21:53:20.603516Z","iopub.status.idle":"2023-12-06T21:53:21.923084Z","shell.execute_reply.started":"2023-12-06T21:53:20.603482Z","shell.execute_reply":"2023-12-06T21:53:21.921606Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shutil.rmtree(\"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:53:51.391727Z","iopub.execute_input":"2023-12-06T21:53:51.392112Z","iopub.status.idle":"2023-12-06T21:53:52.958685Z","shell.execute_reply.started":"2023-12-06T21:53:51.392084Z","shell.execute_reply":"2023-12-06T21:53:52.957239Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\n\ntuned_model = \"codegood/MPhi_New\"\n\ntrainer.model.config.save_pretrained(tuned_model )\ntrainer.save_model(tuned_model)\ntorch.save(model.merge_and_unload().state_dict(), f\"/kaggle/working/{tuned_model}/pytorch_model.bin\")\nmodel.config.use_cache = True\nmodel.eval()\n# wandb.finish()\n\n# trainer.push_to_hub(tuned_model)\nmodel.push_to_hub(tuned_model)\ntokenizer.push_to_hub(tuned_model)\nprint(\"Model saved to Huggingface\")\n\n\n# torch.cuda.empty_cache()\n# torch.cuda.synchronize()\n# trainer.evaluate(metric_key_prefix=\"rouge\")","metadata":{"id":"nKgZBEGVS5a2","execution":{"iopub.status.busy":"2023-12-10T19:51:30.693466Z","iopub.execute_input":"2023-12-10T19:51:30.694326Z","iopub.status.idle":"2023-12-10T19:51:30.952745Z","shell.execute_reply.started":"2023-12-10T19:51:30.694289Z","shell.execute_reply":"2023-12-10T19:51:30.951504Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import HfApi\napi = HfApi()\n\napi.upload_folder(\n    folder_path=\"/kaggle/working/codegood/MPhi_New\",\n    repo_id=\"codegood/MPhi_New\",\n    repo_type=\"model\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:44:34.449226Z","iopub.execute_input":"2023-12-10T17:44:34.449552Z","iopub.status.idle":"2023-12-10T17:45:07.162960Z","shell.execute_reply.started":"2023-12-10T17:44:34.449523Z","shell.execute_reply":"2023-12-10T17:45:07.161809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"Yeshiva University is\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"{prompt}\")\nprint(result[0]['generated_text'])","metadata":{"id":"L7vHP41ITQPb","execution":{"iopub.status.busy":"2023-12-10T03:53:40.576178Z","iopub.execute_input":"2023-12-10T03:53:40.576484Z","iopub.status.idle":"2023-12-10T03:53:54.081612Z","shell.execute_reply.started":"2023-12-10T03:53:40.576457Z","shell.execute_reply":"2023-12-10T03:53:54.080448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"Complete the following statement: AI program at Yeshiva University is\"\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:52:16.628315Z","iopub.execute_input":"2023-12-06T21:52:16.628661Z","iopub.status.idle":"2023-12-06T21:52:38.157987Z","shell.execute_reply.started":"2023-12-06T21:52:16.628634Z","shell.execute_reply":"2023-12-06T21:52:38.156711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"Complete the following statement: Program director for MS AI program at Yeshiva is\"\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:52:47.608127Z","iopub.execute_input":"2023-12-06T21:52:47.608502Z","iopub.status.idle":"2023-12-06T21:52:59.172220Z","shell.execute_reply.started":"2023-12-06T21:52:47.608472Z","shell.execute_reply":"2023-12-06T21:52:59.171323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport random\nfrom rouge_score import rouge_scorer\nindex = random.randint(0, len(sc_data))\n# sys_prompt = \"\"\" \"\"\"\n\ntests, testc = sc_data['test']['text'][index].split(\"[/INST]\")\nprint(\"Sentence:\\n\", tests)\nprint(\"Ground truth completion:\\n\", testc)\n\ngen_ans = pipe(f\"{tests} [/INST]\")[0]['generated_text'].split(\"[/INST]\")[1]\nprint(\"Generated answer:\", gen_ans)\n\nscorer = rouge_scorer.RougeScorer(['rouge1', \"rouge2\", 'rougeL'], use_stemmer=True)\nscores = scorer.score(testc, gen_ans)\nscores","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:54:12.543162Z","iopub.execute_input":"2023-12-06T21:54:12.543562Z","iopub.status.idle":"2023-12-06T21:54:30.547486Z","shell.execute_reply.started":"2023-12-06T21:54:12.543532Z","shell.execute_reply":"2023-12-06T21:54:30.546480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport random\nfrom rouge_score import rouge_scorer\nindex = random.randint(0, len(sc_data))\n# sys_prompt = \"\"\" \"\"\"\n\ntests, testc = sc_data['train']['text'][index].split(\"[/INST]\")\nprint(\"Sentence:\\n\", tests)\nprint(\"Ground truth completion:\\n\", testc)\n\ngen_ans = pipe(f\"{tests} [/INST]\")[0]['generated_text'].split(\"[/INST]\")[1]\nprint(\"Generated answer:\", gen_ans)\n\nscorer = rouge_scorer.RougeScorer(['rouge1', \"rouge2\", 'rougeL'], use_stemmer=True)\nscores = scorer.score(testc, gen_ans)\nscores","metadata":{"execution":{"iopub.status.busy":"2023-12-07T22:02:29.425168Z","iopub.execute_input":"2023-12-07T22:02:29.425974Z","iopub.status.idle":"2023-12-07T22:02:41.213165Z","shell.execute_reply.started":"2023-12-07T22:02:29.425943Z","shell.execute_reply":"2023-12-07T22:02:41.212320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ndef create_df(data, is_qa_data = True):\n    answers, questions, preds = [], [], []\n    rouge1_f, rouge1_prec, rouge1_rec = [], [], []\n    rouge2_f, rouge2_prec, rouge2_rec = [], [], []\n    rougeL_f, rougeL_prec, rougeL_rec = [], [], []\n\n    for qa in tqdm(data):\n        torch.cuda.empty_cache()\n        dat = qa.split(\"[/INST]\")\n        questions.append(dat[0].split(\"[INST]\")[1].split(\":\")[1].strip())\n        answers.append(dat[1][:-4].strip())\n        preds.append(pipe(f\"<s>[INST]{dat[0]}[/INST]\", pad_token_id=tokenizer.eos_token_id)[0]['generated_text'].split(\"[/INST]\")[1])\n        scores = scorer.score(questions[-1], preds[-1])\n        rouge1_prec.append(scores['rouge1'].precision)\n        rouge1_f.append(scores['rouge1'].fmeasure)\n        rouge1_rec.append(scores['rouge1'].recall)\n        rouge2_prec.append(scores['rouge2'].precision)\n        rouge2_f.append(scores['rouge2'].fmeasure)\n        rouge2_rec.append(scores['rouge2'].recall)\n        rougeL_prec.append(scores['rougeL'].precision)\n        rougeL_f.append(scores['rougeL'].fmeasure)\n        rougeL_rec.append(scores['rougeL'].recall)\n\n    df = pd.DataFrame({\"Question\":questions, \"Answer\": answers, \"Prediction\": preds, \"RLFscore\": rougeL_f, \"R2Fscore\": rouge2_f, \"R1Fscore\": rouge1_f,\\\n                    \"RLPrec\": rougeL_prec, \"R2Prec\": rouge2_prec, \"R1Prec\": rouge1_prec, \"RLrecall\": rougeL_rec, \"R2Recall\": rouge2_rec, \"R1Recall\": rouge1_rec})\n    return df\n\n# sc_train_res = create_df(sc_data['train']['text'][:60])\nsc_test_res = create_df(sc_data['test']['text'][:100])","metadata":{"execution":{"iopub.status.busy":"2023-12-07T22:04:58.500245Z","iopub.execute_input":"2023-12-07T22:04:58.501194Z","iopub.status.idle":"2023-12-07T22:04:58.515779Z","shell.execute_reply.started":"2023-12-07T22:04:58.501152Z","shell.execute_reply":"2023-12-07T22:04:58.514568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sc_train_res.to_csv(\"SC_train_res.csv\", index=False)\nsc_test_res.to_csv(\"SC_test_res.csv\", index=False)\n\nprint(\"Sentence completion results\")\nprint(\"Test mean F measures\")\nprint(\"RougeL mean F measure: \", sc_test_res.RLFscore.mean())\nprint(\"Rouge1 mean F measure: \", sc_test_res.R1Fscore.mean())\nprint(\"Rouge2 mean F measure: \", sc_test_res.R2Fscore.mean())\n# print(\"*\"*50)\n# print(\"Train mean F measures\")\n# print(\"RougeL mean F measure: \", sc_train_res.RLFscore.mean())\n# print(\"Rouge1 mean F measure: \", sc_train_res.R1Fscore.mean())\n# print(\"Rouge2 mean F measure: \", sc_train_res.R2Fscore.mean())","metadata":{"execution":{"iopub.status.busy":"2023-12-06T22:27:12.710298Z","iopub.execute_input":"2023-12-06T22:27:12.714236Z","iopub.status.idle":"2023-12-06T22:27:12.792775Z","shell.execute_reply.started":"2023-12-06T22:27:12.714133Z","shell.execute_reply":"2023-12-06T22:27:12.790413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Question answer training","metadata":{}},{"cell_type":"code","source":"import wandb\nwandb.login(key = secret_wandb)\nrun = wandb.init(\n    project='Fine tuning Mistral Instruct 7B QA', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T01:09:29.423909Z","iopub.execute_input":"2023-12-08T01:09:29.424867Z","iopub.status.idle":"2023-12-08T01:09:29.705462Z","shell.execute_reply.started":"2023-12-08T01:09:29.424826Z","shell.execute_reply":"2023-12-08T01:09:29.704195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"per_device_train_batch_size = 5# reduce batch size by 2x if out-of-memory error\nper_device_eval_batch_size = 5\ngradient_accumulation_steps = 1  # increase gradient accumulation steps by 2x if batch size is reduced\noptim = \"paged_adamw_32bit\" # activates the paging for better memory management\nsave_strategy=\"steps\" # checkpoint save strategy to adopt during training\nsave_steps = 500 # number of updates steps before two checkpoint saves\nlogging_steps = 100 # number of update steps between two logs if logging_strategy=\"steps\"\nlearning_rate = 2e-4  # learning rate for AdamW optimizer\nmax_grad_norm = 0.3 # maximum gradient norm (for gradient clipping)\nmax_steps = -1     # training will happen for 320 steps\nwarmup_ratio = 0.05 # number of steps used for a linear warmup from 0 to learning_rate\nlr_scheduler_type = \"cosine\"  # learning rate scheduler\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=per_device_train_batch_size,\n    per_device_eval_batch_size=per_device_eval_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    learning_rate=learning_rate,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n    evaluation_strategy='steps',\n    save_total_limit = 3,\n    num_train_epochs = 1,\n    eval_steps=save_steps,\n    metric_for_best_model=\"eval_loss\",\n    logging_steps=logging_steps,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    report_to=\"tensorboard\",\n    load_best_model_at_end=True,\n)\n\n# Setting sft parameters\ntrainer = SFTTrainer(\n    model=model,\n    eval_dataset=test_qa,\n    train_dataset=train_qa,\n    neftune_noise_alpha=5,\n    peft_config=peft_config,\n    max_seq_length= 256,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:20:11.348972Z","iopub.execute_input":"2023-12-11T12:20:11.349410Z","iopub.status.idle":"2023-12-11T12:20:12.013162Z","shell.execute_reply.started":"2023-12-11T12:20:11.349374Z","shell.execute_reply":"2023-12-11T12:20:12.012018Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:205: UserWarning: You passed a `neftune_noise_alpha` argument to the SFTTrainer, the value you passed will override the one in the `TrainingArguments`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5166 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:247: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(5):\n    torch.cuda.empty_cache()\nmodel.config.use_cache = False\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:20:16.074381Z","iopub.execute_input":"2023-12-11T12:20:16.074799Z","iopub.status.idle":"2023-12-11T12:20:36.600985Z","shell.execute_reply.started":"2023-12-11T12:20:16.074767Z","shell.execute_reply":"2023-12-11T12:20:36.598712Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='21' max='1034' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  21/1034 00:15 < 13:42, 1.23 it/s, Epoch 0.02/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 280\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2734\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2734\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:1905\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1903\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1905\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"model.config.use_cache = True\ntrainer.evaluate(metric_key_prefix=\"rouge\")","metadata":{"execution":{"iopub.status.busy":"2023-12-11T04:17:41.199419Z","iopub.execute_input":"2023-12-11T04:17:41.199792Z","iopub.status.idle":"2023-12-11T04:19:01.122571Z","shell.execute_reply.started":"2023-12-11T04:17:41.199749Z","shell.execute_reply":"2023-12-11T04:19:01.121584Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 01:19]\n    </div>\n    "},"metadata":{}},{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"{'rouge_loss': 1.0005252361297607,\n 'rouge_runtime': 79.9088,\n 'rouge_samples_per_second': 12.514,\n 'rouge_steps_per_second': 1.251}"},"metadata":{}}]},{"cell_type":"code","source":"check_dir = \"/kaggle/working/results\"\nfor i in sorted(os.listdir(check_dir))[:-2]:\n    shutil.rmtree(os.path.join(check_dir, i))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2023-12-11T04:46:30.640292Z","iopub.execute_input":"2023-12-11T04:46:30.640660Z","iopub.status.idle":"2023-12-11T04:46:30.646904Z","shell.execute_reply.started":"2023-12-11T04:46:30.640628Z","shell.execute_reply":"2023-12-11T04:46:30.645628Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"base_model, new_model = \"microsoft/phi-1_5\", \"codegood/MPhi_New\"\nmodel.config.use_cache = True\nmodel.eval()\n\ntrainer.model.save_pretrained(new_model)\ntorch.save(model.merge_and_unload().state_dict(), new_model+\"/pytorch_model.bin\")\n\ndel model, trainer\nfor i in range(20):\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:39:53.314923Z","iopub.execute_input":"2023-12-11T12:39:53.315884Z","iopub.status.idle":"2023-12-11T12:39:58.906916Z","shell.execute_reply.started":"2023-12-11T12:39:53.315850Z","shell.execute_reply":"2023-12-11T12:39:58.905594Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"base_model_reload = AutoModelForCausalLM.from_pretrained(\n    base_model, low_cpu_mem_usage=True, trust_remote_code=True,\n    return_dict=True,torch_dtype=torch.bfloat16,\n    device_map= {\"\": 0})\nmodel = PeftModel.from_pretrained(base_model_reload, new_model)\nmodel = model.merge_and_unload()\nprint(\"Model merged\")\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# os.chdir(\"codegood\")\nmodel.push_to_hub(new_model, check_pr=True)\ntokenizer.push_to_hub(new_model, check_pr=True)\nfrom huggingface_hub import HfApi\napi = HfApi()\n\napi.upload_folder(\n    folder_path=\"/kaggle/working/codegood/MPhi_New\",\n    repo_id=\"codegood/MPhi_Latest\",\n    repo_type=\"model\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T12:40:07.758561Z","iopub.execute_input":"2023-12-11T12:40:07.759229Z","iopub.status.idle":"2023-12-11T12:42:08.009736Z","shell.execute_reply.started":"2023-12-11T12:40:07.759186Z","shell.execute_reply":"2023-12-11T12:42:08.008542Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Model merged\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.84G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0602d1ba03754994b2e1547ef344beb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d7b9be9dcb74390a1b9584b3b7fe48a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d11bbe069ce41a9b213bf7fe3a9a2a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/53.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b928a82de42d4ee689e7a9702f6f1e52"}},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"'https://huggingface.co/codegood/MPhi_Latest/tree/main/'"},"metadata":{}}]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntoke = AutoTokenizer.from_pretrained(\"codegood/MPhi_New\")\nmdl = AutoModelForCausalLM.from_pretrained(\"codegood/MPhi_New\", trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T04:57:22.593943Z","iopub.execute_input":"2023-12-11T04:57:22.594937Z","iopub.status.idle":"2023-12-11T04:57:39.108092Z","shell.execute_reply.started":"2023-12-11T04:57:22.594900Z","shell.execute_reply":"2023-12-11T04:57:39.106626Z"},"trusted":true},"execution_count":104,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[104], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      4\u001b[0m toke \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodegood/MPhi_New\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m mdl \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcodegood/MPhi_New\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3553\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3550\u001b[0m     model \u001b[38;5;241m=\u001b[39m quantizer\u001b[38;5;241m.\u001b[39mpost_init_model(model)\n\u001b[1;32m   3552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _adapter_model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3553\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3554\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_adapter_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3555\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3557\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_loading_info:\n\u001b[1;32m   3561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loading_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/integrations/peft.py:193\u001b[0m, in \u001b[0;36mPeftAdapterMixin.load_adapter\u001b[0;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, adapter_kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_peft_config_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_model_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     adapter_state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_peft_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_model_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# We need to pre-process the state dict to remove unneeded prefixes - for backward compatibility\u001b[39;00m\n\u001b[1;32m    196\u001b[0m processed_adapter_state_dict \u001b[38;5;241m=\u001b[39m {}\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:270\u001b[0m, in \u001b[0;36mload_peft_weights\u001b[0;34m(model_id, device, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find weights for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or in the Hugging Face Hub. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSAFETENSORS_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is present at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m             )\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_safetensors:\n\u001b[0;32m--> 270\u001b[0m     adapters_weights \u001b[38;5;241m=\u001b[39m \u001b[43msafe_load_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     adapters_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(filename, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(device))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/safetensors/torch.py:310\u001b[0m, in \u001b[0;36mload_file\u001b[0;34m(filename, device)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m safe_open(filename, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 310\u001b[0m         result[k] \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.76 GiB total capacity; 13.50 GiB already allocated; 5.75 MiB free; 13.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.76 GiB total capacity; 13.50 GiB already allocated; 5.75 MiB free; 13.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"from huggingface_hub import HfApi\napi = HfApi()\n\napi.upload_folder(\n    folder_path=\"/kaggle/working/codegood/MPhi_New\",\n    repo_id=\"codegood/MPhi_New\",\n    repo_type=\"model\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:19:00.050718Z","iopub.execute_input":"2023-12-10T20:19:00.051583Z","iopub.status.idle":"2023-12-10T20:21:32.510720Z","shell.execute_reply.started":"2023-12-10T20:19:00.051549Z","shell.execute_reply":"2023-12-10T20:21:32.509794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nfrom rouge_score import rouge_scorer\n\nprompt = \"Does Yeshiva university has MS AI program?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200,pad_token_id=tokenizer.eos_token_id)\nresult = pipe(f\"{prompt}\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2023-12-11T03:40:37.256543Z","iopub.execute_input":"2023-12-11T03:40:37.257499Z","iopub.status.idle":"2023-12-11T03:40:44.811748Z","shell.execute_reply.started":"2023-12-11T03:40:37.257461Z","shell.execute_reply":"2023-12-11T03:40:44.810709Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Does Yeshiva university has MS AI program? answer: Yeshiva University has a MS in Artificial Intelligence program. The program is designed to provide students with a comprehensive understanding of AI's applications, ranging from applications in business and finance to applications in science and engineering. The program is fully accredited by the New York State Education Department. The MS in AI is a STEM-approved degree, and students can pursue their careers in business, finance, science, engineering, and other fields. The program is fully online, allowing students to engage with the material at their own pace. The faculty includes experts in the field, including Dr. David Sable, Dr. David Freedman, Dr. David Sable, Dr. David Freedman, Dr. David Sable, Dr. David Sable, Dr. David Sable, Dr. David Sable, Dr. David Sable, Dr. David Sable, Dr. David Sable, Dr. David Sable, Dr. David S\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"What are the requirements for Yeshiva University's MS AI program?\"\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2023-12-11T03:40:55.998292Z","iopub.execute_input":"2023-12-11T03:40:55.998657Z","iopub.status.idle":"2023-12-11T03:41:02.503365Z","shell.execute_reply.started":"2023-12-11T03:40:55.998627Z","shell.execute_reply":"2023-12-11T03:41:02.502098Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"<s>[INST] What are the requirements for Yeshiva University's MS AI program? [/INST] Yeshiva University's MS AI program requires a minimum of 84 credits, including courses in foundational AI and machine learning, as well as specialized courses in neural networks, reinforcement learning, and large language models. answer: Applicants must have a minimum GPA of 3.2 and a minimum SAT score of 1340 or an ACT score of 29 or 32. They should also demonstrate proficiency in Python, Java, C/C++, and SQL/NoSQL. The program's commitment to excellence is reflected in its rigorous academic standards. Applicants interested in the program should submit their admissions application by March 2023. The program's focus on practical skills prepares students for real-world challenges in AI and machine learning. The curriculum covers topics such as data science, programming, and AI ethics, ensuring a well-rounded education. The program's emphasis on hands-on experience, including internships and\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"List out the requirements for Yeshiva University's MS AI program?\"\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2023-12-11T03:41:02.505337Z","iopub.execute_input":"2023-12-11T03:41:02.505662Z","iopub.status.idle":"2023-12-11T03:41:09.033017Z","shell.execute_reply.started":"2023-12-11T03:41:02.505633Z","shell.execute_reply":"2023-12-11T03:41:09.031858Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"<s>[INST] List out the requirements for Yeshiva University's MS AI program? [/INST] Yeshiva University's MS AI program requires a minimum of 128 credits, with courses including foundational courses, AI Track, and specialized courses. answer: The program also offers a minimum of 12 credits of AI-related electives. Students can progress through the program at a pace that suits their needs. The program's focus on AI and machine learning prepares students for careers in technology, business, finance, and academia. The program's emphasis on AI and machine learning reflects the program's commitment to staying at the forefront of AI and machine learning technologies. The program's focus on AI and machine learning reflects the program's commitment to preparing students for careers in technology, business, finance, and academia. The program's focus on AI and machine learning reflects the program's commitment to preparing students for careers in technology, business, finance, and academia. The program's focus on AI and machine learning reflects the\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nprompt = \"How to get accepted to Yeshiva University's MS AI program?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200,pad_token_id=tokenizer.eos_token_id)\nresult = pipe(f\"{prompt}\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2023-12-11T03:41:39.682568Z","iopub.execute_input":"2023-12-11T03:41:39.682959Z","iopub.status.idle":"2023-12-11T03:41:46.462039Z","shell.execute_reply.started":"2023-12-11T03:41:39.682928Z","shell.execute_reply":"2023-12-11T03:41:46.460943Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"How to get accepted to Yeshiva University's MS AI program? High school students with a minimum GPA of 85 and who are accepted to high school may apply. answer: Students who wish to pursue a professional career in AI can opt for the M.S. in Artificial Intelligence and Neural Systems Engineering at the Katz School. The program is designed to provide students with a strong foundation in AI technology and skills, preparing them for leadership roles in the field. The curriculum covers advanced topics in AI, including computer vision, reinforcement learning, and responsible AI. Students engage in hands-on projects, including building AI-powered drones, drones for environmental monitoring, and drones for package delivery. The program also offers a unique opportunity for students to work on AI-related internships, gaining industry experience. The curriculum is structured to be completed in one or two years, providing a fast and efficient pathway to a professional in AI. Students can choose between a full-time or part-time option, allowing flexibility in\nCPU times: user 6.73 s, sys: 42.1 ms, total: 6.77 s\nWall time: 6.77 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nimport random\nindex = random.randint(0, len(test_qa))\ntestQ, testA = test_qa['text'][index].split(\"[/INST]\")\nprint(\"Question:\\n\", testQ)\nprint(\"Ground truth completion:\\n\", testA)\n\ngen_ans = pipe(f\"{testQ}\")[0]['generated_text'].split(\"[/INST]\")[1]\nprint(\"Generated answer:\", gen_ans)\n\nscorer = rouge_scorer.RougeScorer(['rouge1', \"rouge2\", 'rougeL'], use_stemmer=True)\nscores = scorer.score(testA, gen_ans)\nscores","metadata":{"execution":{"iopub.status.busy":"2023-12-10T04:22:54.896317Z","iopub.execute_input":"2023-12-10T04:22:54.897228Z","iopub.status.idle":"2023-12-10T04:22:54.968245Z","shell.execute_reply.started":"2023-12-10T04:22:54.897187Z","shell.execute_reply":"2023-12-10T04:22:54.967046Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nindex = random.randint(0, len(test_qa))\n# sys_prompt = \"\"\" \"\"\"\ntestQ, testA = train_qa['text'][index].split(\"[/INST]\")\nprint(\"Question:\\n\", testQ)\nprint(\"Ground truth completion:\\n\", testA)\n\ngen_ans = pipe(f\"{testQ}\")[0]['generated_text'].split(\"[/INST]\")[1]\nprint(\"Generated answer:\", gen_ans)\n\nscorer = rouge_scorer.RougeScorer(['rouge1', \"rouge2\", 'rougeL'], use_stemmer=True)\nscores = scorer.score(testA, gen_ans)\nscores","metadata":{"execution":{"iopub.status.busy":"2023-12-07T22:03:30.365644Z","iopub.execute_input":"2023-12-07T22:03:30.365929Z","iopub.status.idle":"2023-12-07T22:03:47.195120Z","shell.execute_reply.started":"2023-12-07T22:03:30.365903Z","shell.execute_reply":"2023-12-07T22:03:47.194217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntestQ = \"Can you explain Independent study course in MS AI program?\"\nprint(\"Question: \", testQ)\n\ngen_ans = pipe(f\"<s>[INST]{testQ}[/INST]\")[0]['generated_text'].split(\"[/INST]\")[1]\nprint(\"Generated answer:\", gen_ans)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T00:33:04.612968Z","iopub.execute_input":"2023-12-08T00:33:04.613363Z","iopub.status.idle":"2023-12-08T00:33:24.445947Z","shell.execute_reply.started":"2023-12-08T00:33:04.613332Z","shell.execute_reply":"2023-12-08T00:33:24.444771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# qa_train_res = create_df(train_qa['text'][:60])\n# qa_test_res = create_df(test_qa['text'])","metadata":{"execution":{"iopub.status.busy":"2023-12-07T22:08:25.474492Z","iopub.execute_input":"2023-12-07T22:08:25.474872Z","iopub.status.idle":"2023-12-07T22:08:25.479272Z","shell.execute_reply.started":"2023-12-07T22:08:25.474837Z","shell.execute_reply":"2023-12-07T22:08:25.478234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test = test_qa.to_pandas()\n# test['question'] = test.text.apply(lambda x:x.split(\"[/INST]\")[0].split(\"[INST]\")[1].split(\":\")[1])\n# test['answer'] = test['text'].apply(lambda x:x.split(\"[/INST]\")[1])","metadata":{"execution":{"iopub.status.busy":"2023-12-10T17:24:06.195287Z","iopub.execute_input":"2023-12-10T17:24:06.195671Z","iopub.status.idle":"2023-12-10T17:24:06.200144Z","shell.execute_reply.started":"2023-12-10T17:24:06.195622Z","shell.execute_reply":"2023-12-10T17:24:06.199224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_qa.to_pandas()\ntest['question'] = test.text.apply(lambda x:x.split(\":\")[1][:-6])\ntest['answer'] = test['text'].apply(lambda x:x.split(\":\")[2])","metadata":{"execution":{"iopub.status.busy":"2023-12-11T03:42:18.186551Z","iopub.execute_input":"2023-12-11T03:42:18.187389Z","iopub.status.idle":"2023-12-11T03:42:18.230864Z","shell.execute_reply.started":"2023-12-11T03:42:18.187347Z","shell.execute_reply":"2023-12-11T03:42:18.229838Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T03:42:45.309769Z","iopub.execute_input":"2023-12-11T03:42:45.310139Z","iopub.status.idle":"2023-12-11T03:42:45.316095Z","shell.execute_reply.started":"2023-12-11T03:42:45.310111Z","shell.execute_reply":"2023-12-11T03:42:45.314978Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"%%time\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200,pad_token_id=tokenizer.eos_token_id, batch_size=15, trust_remote_code=True)\nquestions = test.question.apply(lambda x:f\"{x}\").to_list()\ngenerated_ans = pipe(questions)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T03:43:17.553546Z","iopub.execute_input":"2023-12-11T03:43:17.553957Z","iopub.status.idle":"2023-12-11T04:03:01.349253Z","shell.execute_reply.started":"2023-12-11T03:43:17.553925Z","shell.execute_reply":"2023-12-11T04:03:01.347946Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"CPU times: user 14min 38s, sys: 5min 6s, total: 19min 44s\nWall time: 19min 43s\n","output_type":"stream"}]},{"cell_type":"code","source":"generated_answers = [i[0]['generated_text'].split(\":\")[-1] for i in generated_ans]","metadata":{"execution":{"iopub.status.busy":"2023-12-11T04:11:46.952329Z","iopub.execute_input":"2023-12-11T04:11:46.953251Z","iopub.status.idle":"2023-12-11T04:11:46.961538Z","shell.execute_reply.started":"2023-12-11T04:11:46.953215Z","shell.execute_reply":"2023-12-11T04:11:46.960029Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"test['generated'] = generated_answers\ntest = test[test['generated'] != \"\"]\ntest.to_csv(\"Test_preds_MPhi.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T04:14:48.530798Z","iopub.execute_input":"2023-12-11T04:14:48.531656Z","iopub.status.idle":"2023-12-11T04:14:48.596894Z","shell.execute_reply.started":"2023-12-11T04:14:48.531623Z","shell.execute_reply":"2023-12-11T04:14:48.595843Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"from rouge import Rouge\nfrom tabulate import tabulate\nrouge = Rouge()\n\nscores = rouge.get_scores(test.answer.to_list(), test.generated.to_list(), avg=True)\n\nscore_table = [{'Metric': metric, 'Precision': score['p'], 'Recall': score['r'], 'F1-Score': score['f']} for metric, score in scores.items()]\n\nprint(tabulate(score_table, headers='keys', tablefmt='grid'))","metadata":{"execution":{"iopub.status.busy":"2023-12-11T04:14:05.856098Z","iopub.execute_input":"2023-12-11T04:14:05.857113Z","iopub.status.idle":"2023-12-11T04:14:09.224808Z","shell.execute_reply.started":"2023-12-11T04:14:05.857066Z","shell.execute_reply":"2023-12-11T04:14:09.223833Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"+----------+-------------+-----------+------------+\n| Metric   |   Precision |    Recall |   F1-Score |\n+==========+=============+===========+============+\n| rouge-1  |    0.393863 | 0.147787  |  0.195931  |\n+----------+-------------+-----------+------------+\n| rouge-2  |    0.162256 | 0.0440985 |  0.0620082 |\n+----------+-------------+-----------+------------+\n| rouge-l  |    0.377279 | 0.140092  |  0.186507  |\n+----------+-------------+-----------+------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"qa_test_res.to_csv(\"QA_test_res.csv\", index=False)\nqa_train_res.to_csv(\"QA_train_res.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test['generated'] = test.generated.apply(lambda x: x[0]['generated_text'].split(\"[/INST]\")[1])\nfrom rouge import Rouge\nfrom tabulate import tabulate\n\nact_ans = []\npred_ans = []\n\nfor index, row in test.iterrows():\n    act_ans.append(row['answer'])\n    pred_ans.append(row['generated'])\n\nhyps, refs = map(list, (pred_ans,act_ans))\nrouge = Rouge()\n\nscores = rouge.get_scores(hyps, refs, avg=True)\n\nscore_table = [{'Metric': metric, 'Precision': score['p'], 'Recall': score['r'], 'F1-Score': score['f']} for metric, score in scores.items()]\n\nprint(tabulate(score_table, headers='keys', tablefmt='grid'))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T22:52:10.189974Z","iopub.execute_input":"2023-12-07T22:52:10.190360Z","iopub.status.idle":"2023-12-07T22:52:12.559174Z","shell.execute_reply.started":"2023-12-07T22:52:10.190327Z","shell.execute_reply":"2023-12-07T22:52:12.558184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.generated = test.generated.apply(lambda x: x[0]['generated_text'].split(\"[/INST]\")[1])","metadata":{"execution":{"iopub.status.busy":"2023-12-07T22:52:06.063628Z","iopub.execute_input":"2023-12-07T22:52:06.064028Z","iopub.status.idle":"2023-12-07T22:52:06.071413Z","shell.execute_reply.started":"2023-12-07T22:52:06.063995Z","shell.execute_reply":"2023-12-07T22:52:06.070553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom sparseml.pytorch.models import mnist_net\nfrom sparseml.pytorch.utils import ModuleExporter\n\nexporter = ModuleExporter(model, output_dir=os.path.join(\".\", \"onnx-export\"))\nexporter.export_onnx(sample_batch=torch.randn(4, 256, dtype=torch.int))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T23:57:19.703425Z","iopub.execute_input":"2023-12-07T23:57:19.703876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
